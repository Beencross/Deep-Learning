{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset.ipynb","provenance":[],"authorship_tag":"ABX9TyOUWJbojn2PiK2R5Z9zHrOC"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"R5kCvuTiv9SC"},"source":["import cv2\n","import logging\n","import math\n","import numpy as np\n","import pandas as pd\n","import random\n","from collections import defaultdict\n","from itertools import chain\n","from operator import itemgetter\n","from pathlib import Path\n","\n","import torch\n","import torchvision.transforms.functional as F\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","import precomputed as P\n","\n","### Augments all images in a batch and retuns a list of augmented batches ### \n","def tta(args, images):\n","\n","    ret = []\n","    n1 = math.ceil(args.tta ** 0.5)   # exponent\n","    n2 = math.ceil(args.tta / n1)\n","    k = 0\n","    for i in range(n1):\n","        for j in range(n2):\n","            if k >= args.tta:\n","                break\n","\n","            dw = round(args.tta_size * images.size(2))\n","            dh = round(args.tta_size * images.size(3))\n","            w = i * (images.size(2) - dw) // max(n1 - 1, 1)\n","            h = j * (images.size(3) - dh) // max(n2 - 1, 1)\n","\n","            ### images that will be returned: they will be the same images as the input but randomly modified (horizontal and vertical flip, 90 degrees rotation...)\n","            imgs = images[:, :, w:w + dw, h:h + dh]\n","            if k & 1:\n","                imgs = imgs.flip(3)           # flips on axis\n","            if k & 2:\n","                imgs = imgs.flip(2)\n","            if k & 4:\n","                imgs = imgs.transpose(2, 3)   # permute dimensions\n","\n","            ### sampling the input to the given size\n","            ret.append(nn.functional.interpolate(imgs, images.size()[2:], mode='nearest'))\n","            k += 1\n","\n","    return ret\n","\n","def worker_init_fn(worker_id):\n","    np.random.seed(random.randint(0, 10 ** 9) + worker_id)\n","\n","def get_train_val_loader(args, predict=False):\n","    #### applying 1st transformation on images : \n","    def train_transform1(image):\n","        ### applying random horizontal and vertical flips and 90Â° rotations:\n","        if random.random() < 0.5:\n","            image = image[:, ::-1, :]\n","        if random.random() < 0.5:\n","            image = image[::-1, :, :]\n","        if random.random() < 0.5:\n","            image = image.transpose([1, 0, 2])\n","        image = np.ascontiguousarray(image)\n","\n","        ### applying random resizing to the images : \n","        if args.scale_aug != 1:\n","            size = random.randint(round(512 * args.scale_aug), 512)\n","            x = random.randint(0, 512 - size)\n","            y = random.randint(0, 512 - size)\n","            image = image[x:x + size, y:y + size]\n","            image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_NEAREST)\n","\n","        return image\n","\n","    ### applying 2nd transformation on images : \n","    def train_transform2(image):\n","        ### random samples from a normal distribution\n","        a, b = np.random.normal(1, args.pw_aug[0], (6, 1, 1)), np.random.normal(0, args.pw_aug[1], (6, 1, 1))\n","        a, b = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n","        return image * a + b\n","\n","    # training data loader : \n","    if not predict:\n","        train_dataset = CellularDataset(args.data, 'train_all_controls' if args.all_controls_train else 'train_controls',\n","                transform=(train_transform1, train_transform2), cv_number=args.cv_number,\n","                split_seed=args.data_split_seed, normalization=args.data_normalization)\n","        train = DataLoader(train_dataset, args.batch_size, shuffle=True, drop_last=True,\n","                num_workers=args.num_data_workers, worker_init_fn=worker_init_fn)\n","\n","    ### if in training, split dataset between train / val :\n","    for i in range(1 if not predict else 2):\n","        dataset = CellularDataset(args.data, 'val' if i == 0 else 'train', cv_number=args.cv_number,\n","                split_seed=args.data_split_seed, normalization=args.data_normalization)\n","        loader = DataLoader(dataset, args.batch_size, shuffle=False, num_workers=args.num_data_workers,\n","                worker_init_fn=worker_init_fn)\n","        if i == 0:\n","            val = loader\n","        else:\n","            train = loader\n","\n","    assert len(set(train.dataset.data).intersection(set(val.dataset.data))) == 0\n","    return train, val\n","\n","# testing data loader : \n","def get_test_loader(args, exclude_leak=False):\n","    test_dataset = CellularDataset(args.data, 'test' if not exclude_leak else 'test_noleak',\n","            normalization=args.data_normalization)\n","    return DataLoader(test_dataset, args.batch_size, shuffle=False, num_workers=args.num_data_workers,\n","            worker_init_fn=worker_init_fn)\n","\n","\n","class CellularDataset(Dataset):\n","    treatment_classes = 1108\n","\n","    def __init__(self, root_dir, mode, split_seed=0, cv_number=0, transform=None, normalization='global'):\n","        \"\"\"\n","        :param split_seed: seed for train/val split of labeled experiments and HUVEC-18\n","        :param mode: possible choices:\n","                        train -- dataset containing only non-control images from training set\n","                        train_controls -- dataset containing non-control and control images from training set\n","                        train_all_controls -- dataset containing non-control and control images from training set and\n","                                              control images from validation and test set\n","                        val -- dataset containing only non-control images from validation set\n","                        test -- dataset containing only non-control images from test set\n","                        test_noleak -- dataset containing only non-control images from test set excluding HUVEC-18\n","        :param transform: tuple of 2 functions for image transformation. First is called right after loading with image\n","                          in numpy format. Second is called after normalization and converting to tensor\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        self.root = Path(root_dir)\n","        self.transform = transform\n","\n","        assert normalization in ['global', 'experiment', 'sample']\n","        self.normalization = normalization\n","\n","        ## setting \"mode\" variable to \"train\", \"test\", or \"val\" values only \n","        ## if mode == (train , val , test) => it doesn't change ;\n","        ## else : \n","        if mode == 'train_controls':\n","            mode = 'train'\n","            move_controls = True\n","            all_controls = False\n","        elif mode == 'train_all_controls':\n","            mode = 'train'\n","            move_controls = True\n","            all_controls = True\n","        else:\n","            move_controls = False\n","            all_controls = False\n","\n","        if mode == 'test_noleak':\n","            mode = 'test'\n","            exclude_leak = True\n","        else:\n","            exclude_leak = False\n","        # checking that \"mode\" = train , val or test\n","        assert mode in ['train', 'val', 'test']\n","        self.mode = mode\n","\n","        csv = pd.read_csv(self.root / ('train.csv' if mode in ['train', 'val'] else 'test.csv'))\n","        csv_controls = pd.read_csv(self.root / ('train_controls.csv' if mode in ['train', 'val'] else 'test_controls.csv'))\n","        if all_controls:\n","            csv_controls_test = pd.read_csv(self.root / 'test_controls.csv')\n","        self.data = []  # data = (experiment, plate, well, site, cell_type, sirna or None)\n","        experiments = {}   \n","        # going through all files\n","        for row in chain(csv.iterrows(), csv_controls.iterrows(), *([csv_controls_test.iterrows()] if all_controls else [])):\n","            r = row[1]\n","            # finding the typ of the data : \n","            typ = r.experiment[:r.experiment.find('-')]\n","            # filling the data attribute with each element in file : \n","            self.data.append((r.experiment, r.plate, r.well, 1, typ, r.sirna if hasattr(r, 'sirna') else None))\n","            self.data.append((r.experiment, r.plate, r.well, 2, typ, r.sirna if hasattr(r, 'sirna') else None))\n","            if not hasattr(r, 'sirna') or r.sirna < self.treatment_classes:\n","                # adding all experiments to the \"experiments\" dictionary :\n","                if typ not in experiments:\n","                    experiments[typ] = set()\n","                experiments[typ].add(r.experiment)\n","        if mode in ['train', 'val']:\n","            data_dict = {(e, p, w): sir for e, p, w, s, typ, sir in self.data}\n","            for row in pd.read_csv(self.root / 'test.csv').iterrows():\n","                r = row[1]\n","                # finding the typ of the data : \n","                typ = r.experiment[:r.experiment.find('-')]\n","                # HUVEC-18 is a known leak ; so it's moved to the training set (to the data attribute): \n","                if r.experiment == 'HUVEC-18':\n","                    sirna = data_dict[('RPE-03', (r.plate - 2) % 4 + 1, r.well)]\n","                    assert sirna < self.treatment_classes\n","                    self.data.append((r.experiment, r.plate, r.well, 1, typ, sirna))\n","                    self.data.append((r.experiment, r.plate, r.well, 2, typ, sirna))\n","                    # adding all experiments to the \"experiments\" dictionary :\n","                    if typ not in experiments:\n","                        experiments[typ] = set()\n","                    experiments[typ].add(r.experiment)\n","            if not all_controls:\n","                for row in pd.read_csv(self.root / 'test_controls.csv').iterrows():\n","                    r = row[1]\n","                    typ = r.experiment[:r.experiment.find('-')]\n","                    if r.experiment == 'HUVEC-18':\n","                        sirna = data_dict[('RPE-03', (r.plate - 2) % 4 + 1, r.well)]\n","                        assert sirna == r.sirna or sirna == 1138 or r.sirna == 1138\n","                        self.data.append((r.experiment, r.plate, r.well, 1, typ, r.sirna))\n","                        self.data.append((r.experiment, r.plate, r.well, 2, typ, r.sirna))\n","        ## filters the data attribute from HUVEC-18 experiments \n","        if exclude_leak:\n","            self.data = list(filter(lambda x: x[0] != 'HUVEC-18', self.data))\n","\n","        self.cell_types = sorted(experiments.keys())\n","        all_data = self.data.copy()\n","\n","        if mode != 'test':\n","            state = random.Random(split_seed)\n","            # cells = experiments.items()[1] : \n","            # len(cells) = len(experiments.items())\n","            cells = list(map(itemgetter(1), sorted(experiments.items())))\n","            for i in range(len(cells)):\n","                cells[i] = sorted(cells[i])\n","                if i == 3:\n","                    cells[i] = cells[i] + cells[i]  # duplicate U2OS experiments for validation\n","                state.shuffle(cells[i])\n","\n","            # cell[i] is a list of experiments for i-th cell type\n","            assert list(map(len, cells)) == [7, 17, 7, 6]\n","\n","            # counts of experiments from given cell type for given fold\n","            ### logging info : \n","            counts = [\n","                [2, 2, 1, 1],\n","                [1, 3, 2, 1],\n","                [1, 3, 1, 1],\n","                [1, 3, 1, 1],\n","                [1, 3, 1, 1],\n","                [1, 3, 1, 1],\n","            ]\n","\n","            splits = []\n","            start = [0, 0, 0, 0]\n","            for count in counts:\n","                splits.append(sorted(cells[0][start[0]:start[0] + count[0]]) +\n","                              sorted(cells[1][start[1]:start[1] + count[1]]) +\n","                              sorted(cells[2][start[2]:start[2] + count[2]]) +\n","                              sorted(cells[3][start[3]:start[3] + count[3]]))\n","                for i in range(4):\n","                    start[i] += count[i]\n","            assert start == [7, 17, 7, 6]\n","            logging.info('Splits: {}'.format(splits))\n","\n","            if cv_number != -1:\n","                val = sorted(splits[cv_number])\n","            else:\n","                val = []\n","            all = []\n","            for k, v in sorted(experiments.items()):\n","                v = sorted(v)\n","                all.extend(v)\n","            tr = sorted(set(all) - set(val))\n","\n","            if mode == 'train':\n","                logging.info('Train dataset: {}'.format(sorted(tr)))\n","                self.data = list(filter(lambda d: d[0] in tr, self.data))\n","            elif mode == 'val':\n","                logging.info('Val dataset: {}'.format(val))\n","                self.data = list(filter(lambda d: d[0] in val, self.data))\n","            else:\n","                assert 0\n","\n","        assert len(set(self.data)) == len(self.data)\n","        assert len(set(all_data)) == len(all_data)\n","\n","        controls = list(filter(lambda d: d[-1] is not None and d[-1] >= self.treatment_classes,\n","            (all_data if all_controls else self.data)))\n","        self.data = list(filter(lambda d: not (d[-1] is not None and d[-1] >= self.treatment_classes),\n","            self.data))\n","        if move_controls:\n","            self.data += controls\n","\n","        self.filter()\n","\n","        logging.info('{} dataset size: data: {}'.format(mode, len(self.data)))\n","  \n","    ### Filter dataset by given function\n","    def filter(self, func=None):\n","        if func is None:\n","            self.data_indices = None\n","        else:\n","            self.data_indices = list(filter(lambda i: func(i, self.data[i]), range(len(self.data))))\n","\n","    ### dunder function : returns length of data (elements)\n","    def __len__(self):\n","        return len(self.data_indices if self.data_indices is not None else self.data)\n","\n","    ### dunder function, returns tuple corresponding to the image in index i : \n","    def __getitem__(self, i):\n","        i = self.data_indices[i] if self.data_indices is not None else i\n","        d = self.data[i]\n","\n","        images = []\n","        for channel in range(1, 7):\n","            for dir in ['train', 'test']:\n","                path = self.root / dir / d[0] / 'Plate{}'.format(d[1]) / '{}_s{}_w{}.png'.format(d[2], d[3], channel)\n","                if path.exists():\n","                    break\n","            else:\n","                assert 0\n","            images.append(cv2.imread(str(path), cv2.IMREAD_GRAYSCALE))\n","            assert images[-1] is not None\n","        ### Loading images in numpy format\n","        image = np.stack(images, axis=-1)\n","\n","        ### first transformation\n","        if self.transform is not None:\n","            image = self.transform[0](image)\n","        ### converting image to tensor\n","        image = F.to_tensor(image)\n","\n","        ### Normalizing each image channel to N(0, 1)\n","        if self.normalization == 'experiment':\n","            pixel_mean = torch.tensor(P.pixel_stats[d[0]][0]) / 255\n","            pixel_std = torch.tensor(P.pixel_stats[d[0]][1]) / 255\n","        elif self.normalization == 'global':\n","            pixel_mean = torch.tensor(list(map(lambda x: x[0], P.pixel_stats.values()))).mean(0) / 255\n","            pixel_std = torch.tensor(list(map(lambda x: x[1], P.pixel_stats.values()))).mean(0) / 255\n","        elif self.normalization == 'sample':\n","            pixel_mean = image.mean([1, 2])\n","            pixel_std = image.std([1, 2]) + 1e-8\n","        else:\n","            assert 0\n","\n","        image = (image - pixel_mean.reshape(-1, 1, 1)) / pixel_std.reshape(-1, 1, 1)\n","\n","        ### second transformation\n","        if self.transform is not None:\n","            image = self.transform[1](image)\n","\n","        cell_type = nn.functional.one_hot(torch.tensor(self.cell_types.index(d[-2]), dtype=torch.long),\n","                len(self.cell_types)).float()\n","\n","        r = [image, cell_type, torch.tensor(i, dtype=torch.long)]\n","        if self.mode != 'test':\n","            r.append(torch.tensor(d[-1], dtype=torch.long))\n","        return tuple(r)"],"execution_count":null,"outputs":[]}]}