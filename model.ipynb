{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1606332067445,
     "user": {
      "displayName": "Dabbous TALA",
      "photoUrl": "",
      "userId": "07073395752952295616"
     },
     "user_tz": -60
    },
    "id": "L55tu7E3-ij5",
    "outputId": "517f82c7-c30f-48eb-801a-b4a2e09e351e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 1., 5.])"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        kwargs = {}\n",
    "        backbone = args.backbone\n",
    "        #### backbone = modele #### \n",
    "        if args.backbone.startswith('mem-'):\n",
    "            kwargs['memory_efficient'] = True\n",
    "            backbone = args.backbone[4:]\n",
    " \n",
    "        if backbone.startswith('densenet'):\n",
    "        #### if the model is a densenet : we create a convolution network with 6 input channels ####\n",
    "        #### we retrieve the pre-trained model, the features, and the number of features ####\n",
    "            channels = 96 if backbone == 'densenet161' else 64\n",
    "            first_conv = nn.Conv2d(6, channels, 7, 2, 3, bias=False)\n",
    "            pretrained_backbone = getattr(torchvision.models, backbone)(pretrained=True, **kwargs)\n",
    "            self.features = pretrained_backbone.features\n",
    "            self.features.conv0 = first_conv\n",
    "            features_num = pretrained_backbone.classifier.in_features\n",
    "         \n",
    "        elif backbone.startswith('resnet') or backbone.startswith('resnext'):\n",
    "        #### if the model is a resnet or resnext : we create a network composed of a convolution, a normalisation layer, a relu layer, a maxpool layer followed by layers 1, 2, 3 and 4 of the model ####\n",
    "        #### we retrieve the pre-trained model, the features, and the number of features ####\n",
    "            first_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)\n",
    "            pretrained_backbone = getattr(torchvision.models, backbone)(pretrained=True, **kwargs)\n",
    "            self.features = nn.Sequential(\n",
    "                first_conv,\n",
    "                pretrained_backbone.bn1,\n",
    "                pretrained_backbone.relu,\n",
    "                pretrained_backbone.maxpool,\n",
    "                pretrained_backbone.layer1,\n",
    "                pretrained_backbone.layer2,\n",
    "                pretrained_backbone.layer3,\n",
    "                pretrained_backbone.layer4,\n",
    "            )\n",
    "            features_num = pretrained_backbone.fc.in_features\n",
    "        \n",
    "        elif backbone.startswith('efficientnet'):\n",
    "        #### if the model is a efficientnet : we create a convolution network with 6 input channels ####\n",
    "        #### we retrieve the pre-trained model, the features, and the number of features ####\n",
    "            from efficientnet_pytorch import EfficientNet\n",
    "            self.efficientnet = EfficientNet.from_pretrained(backbone)\n",
    "            first_conv = nn.Conv2d(6, self.efficientnet._conv_stem.out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "            self.efficientnet._conv_stem = first_conv\n",
    "            self.features = self.efficientnet.extract_features\n",
    "            features_num = self.efficientnet._conv_head.out_channels\n",
    "        else:\n",
    "            raise ValueError('wrong backbone')\n",
    "\n",
    "        self.concat_cell_type = args.concat_cell_type\n",
    "        self.classes = args.classes\n",
    "\n",
    "        #### if concat_cell_type = TRUE, we add 4 to the number of features because we concatenate the elements of the Id matrix(4x4) to the output of the GAP ####\n",
    "        features_num = features_num + (4 if self.concat_cell_type else 0)\n",
    "\n",
    "        #### network \"neck\" : \n",
    "        #### nn.BatchNorm1d : data normalisation, \n",
    "        #### nn.Linear : linear transformation that adapts the layer's parameters, \n",
    "        #### nn.ReLu : replaces negative values by 0 ####\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.BatchNorm1d(features_num),\n",
    "            nn.Linear(features_num, args.embedding_size, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(args.embedding_size),\n",
    "            nn.Linear(args.embedding_size, args.embedding_size, bias=False),\n",
    "            nn.BatchNorm1d(args.embedding_size),\n",
    "        )\n",
    "        #### number of outputs from the network : \"embedding_size\" \n",
    "\n",
    "        self.arc_margin_product = ArcMarginProduct(args.embedding_size, args.classes)\n",
    "        \n",
    "        #### network \"head\" : \n",
    "        #### if the network doesn't have hidden layers, it will be made of a linear layer where the input is the output of the neck, and the output is the prediction  ####\n",
    "        if args.head_hidden is None:\n",
    "            self.head = nn.Linear(args.embedding_size, args.classes)\n",
    "        #### else, head will be made of hidden layers, every one of them made of the following layers : Linear, BatchNorm1d, ReLu ####\n",
    "        else:\n",
    "            self.head = []\n",
    "            for input_size, output_size in zip([args.embedding_size] + args.head_hidden, args.head_hidden):\n",
    "                self.head.extend([\n",
    "                    nn.Linear(input_size, output_size, bias=False),\n",
    "                    nn.BatchNorm1d(output_size),\n",
    "                    nn.ReLU(),\n",
    "                ])\n",
    "            self.head.append(nn.Linear(args.head_hidden[-1], args.classes))\n",
    "            self.head = nn.Sequential(*self.head)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
    "                m.momentum = args.bn_mom\n",
    "\n",
    "    #### GAP Bloc \n",
    "    #### input will be x (output of the densenet) and s (cell type onehot), \n",
    "    #### replaces x by its mean values and concatenates x to s, \n",
    "    #### gives (x+s) as input to neck,\n",
    "    #### returns the output of the neck ####\n",
    "    def embed(self, x, s):\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        #### replaces x by its mean values x.shape in output = [1,1] ####\n",
    "        #### flatten x : \n",
    "        x = x.view(x.size(0), -1)\n",
    "        #### if concat_cell_type = TRUE, concatenate x and s following (dim=1) (columns)\n",
    "        if self.concat_cell_type:\n",
    "            x = torch.cat([x, s], dim=1)\n",
    "\n",
    "        embedding = self.neck(x)\n",
    "        return embedding\n",
    "\n",
    "    def metric_classify(self, embedding):\n",
    "        return self.arc_margin_product(embedding)\n",
    "\n",
    "    def classify(self, embedding):\n",
    "        return self.head(embedding)\n",
    "\n",
    "\n",
    "class ModelAndLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.model = Model(args)\n",
    "        self.metric_crit = ArcFaceLoss()\n",
    "        self.crit = DenseCrossEntropy()\n",
    "\n",
    "    def train_forward(self, x, s, y):\n",
    "        #### embedding = output of the neck \n",
    "        embedding = self.model.embed(x, s)\n",
    "\n",
    "        metric_output = self.model.metric_classify(embedding) #### returns arc_margin_product(embedding)\n",
    "        metric_loss = self.metric_crit(metric_output, y) #### ArcFaceLoss( arc_margin_product(embedding), y )\n",
    "\n",
    "        output = self.model.classify(embedding) #### output of the head \n",
    "        loss = self.crit(output, y)\n",
    "\n",
    "        #### accuracy calculation : calculates the mean value of good predictions of the model ####\n",
    "        #### output.max(1)[1] = index (of the column) of the maximum value of the row ####\n",
    "        acc = (output.max(1)[1] == y.max(1)[1]).float().mean().item()\n",
    "\n",
    "        #### returns the loss and the accuracy of the model #### \n",
    "        coeff = self.args.metric_loss_coeff\n",
    "        return loss * (1 - coeff) + metric_loss * coeff, acc\n",
    "\n",
    "    def eval_forward(self, x, s):\n",
    "        embedding = self.model.embed(x, s)\n",
    "        output = self.model.classify(embedding)\n",
    "        return output\n",
    "\n",
    "    def embed(self, x, s):\n",
    "        return self.model.embed(x, s)\n",
    "\n",
    "\n",
    "class DenseCrossEntropy(nn.Module):\n",
    "    def forward(self, x, target):\n",
    "        x = x.float()\n",
    "        target = target.float()\n",
    "        #### apply log_softmax to x following the dimension -1  (the last dimension, columns) <=> sum of the elements on each row = 1\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1) \n",
    "        #### multiply (-logprobs) and target to get only the value of each correct class predicted \n",
    "        #### we add the values of the same row \n",
    "        #### we calculate the mean value of \"loss\"\n",
    "        loss = -logprobs * target\n",
    "        loss = loss.sum(-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class ArcFaceLoss(nn.modules.Module):     ### Loss function ###\n",
    "    def __init__(self, s=30.0, m=0.5):\n",
    "        super().__init__()\n",
    "        self.crit = DenseCrossEntropy()\n",
    "        self.s = s\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
    "        output *= self.s\n",
    "        loss = self.crit(output, labels)\n",
    "        return loss / 2\n",
    "\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features): \n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    #### reset_parameters => normalise data ####\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        #### weight.shape = [out_features, in_features]\n",
    "        #### weight.size(1) => size of the dimension 1 => out_features\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        #### fill out weight with the values of its uniform distribution \n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        return cosine"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
