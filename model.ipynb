{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L55tu7E3-ij5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517f82c7-c30f-48eb-801a-b4a2e09e351e"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        kwargs = {}\n",
        "        backbone = args.backbone\n",
        "        #### backbone = modele #### \n",
        "        if args.backbone.startswith('mem-'):\n",
        "            kwargs['memory_efficient'] = True\n",
        "            backbone = args.backbone[4:]\n",
        " \n",
        "        if backbone.startswith('densenet'):\n",
        "        #### if the model is a densenet : we create a convolution network with 6 input channels ####\n",
        "        #### we retrieve the pre-trained model, the features, and the number of features ####\n",
        "            channels = 96 if backbone == 'densenet161' else 64\n",
        "            first_conv = nn.Conv2d(6, channels, 7, 2, 3, bias=False)\n",
        "            pretrained_backbone = getattr(torchvision.models, backbone)(pretrained=True, **kwargs)\n",
        "            self.features = pretrained_backbone.features\n",
        "            self.features.conv0 = first_conv\n",
        "            features_num = pretrained_backbone.classifier.in_features\n",
        "         \n",
        "        elif backbone.startswith('resnet') or backbone.startswith('resnext'):\n",
        "        #### if the model is a resnet or resnext : we create a network composed of a convolution, a normalisation layer, a relu layer, a maxpool layer followed by layers 1, 2, 3 and 4 of the model ####\n",
        "        #### we retrieve the pre-trained model, the features, and the number of features ####\n",
        "            first_conv = nn.Conv2d(6, 64, 7, 2, 3, bias=False)\n",
        "            pretrained_backbone = getattr(torchvision.models, backbone)(pretrained=True, **kwargs)\n",
        "            self.features = nn.Sequential(\n",
        "                first_conv,\n",
        "                pretrained_backbone.bn1,\n",
        "                pretrained_backbone.relu,\n",
        "                pretrained_backbone.maxpool,\n",
        "                pretrained_backbone.layer1,\n",
        "                pretrained_backbone.layer2,\n",
        "                pretrained_backbone.layer3,\n",
        "                pretrained_backbone.layer4,\n",
        "            )\n",
        "            features_num = pretrained_backbone.fc.in_features\n",
        "        \n",
        "        elif backbone.startswith('efficientnet'):\n",
        "        #### if the model is a efficientnet : we create a convolution network with 6 input channels ####\n",
        "        #### we retrieve the pre-trained model, the features, and the number of features ####\n",
        "            from efficientnet_pytorch import EfficientNet\n",
        "            self.efficientnet = EfficientNet.from_pretrained(backbone)\n",
        "            first_conv = nn.Conv2d(6, self.efficientnet._conv_stem.out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "            self.efficientnet._conv_stem = first_conv\n",
        "            self.features = self.efficientnet.extract_features\n",
        "            features_num = self.efficientnet._conv_head.out_channels\n",
        "        else:\n",
        "            raise ValueError('wrong backbone')\n",
        "\n",
        "        self.concat_cell_type = args.concat_cell_type\n",
        "        self.classes = args.classes\n",
        "\n",
        "        #### if concat_cell_type = TRUE, we add 4 to the number of features because we concatenate the elements of the Id matrix(4x4) to the output of the GAP ####\n",
        "        features_num = features_num + (4 if self.concat_cell_type else 0)\n",
        "\n",
        "        #### network \"neck\" : \n",
        "        #### nn.BatchNorm1d : data normalisation, \n",
        "        #### nn.Linear : linear transformation that adapts the layer's parameters, \n",
        "        #### nn.ReLu : replaces negative values by 0 ####\n",
        "        self.neck = nn.Sequential(\n",
        "            nn.BatchNorm1d(features_num),\n",
        "            nn.Linear(features_num, args.embedding_size, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(args.embedding_size),\n",
        "            nn.Linear(args.embedding_size, args.embedding_size, bias=False),\n",
        "            nn.BatchNorm1d(args.embedding_size),\n",
        "        )\n",
        "        #### number of outputs from the network : \"embedding_size\" \n",
        "\n",
        "        self.arc_margin_product = ArcMarginProduct(args.embedding_size, args.classes)\n",
        "        \n",
        "        #### network \"head\" : \n",
        "        #### if the network doesn't have hidden layers, it will be made of a linear layer where the input is the output of the neck, and the output is the prediction  ####\n",
        "        if args.head_hidden is None:\n",
        "            self.head = nn.Linear(args.embedding_size, args.classes)\n",
        "        #### else, head will be made of hidden layers, every one of them made of the following layers : Linear, BatchNorm1d, ReLu ####\n",
        "        else:\n",
        "            self.head = []\n",
        "            for input_size, output_size in zip([args.embedding_size] + args.head_hidden, args.head_hidden):\n",
        "                self.head.extend([\n",
        "                    nn.Linear(input_size, output_size, bias=False),\n",
        "                    nn.BatchNorm1d(output_size),\n",
        "                    nn.ReLU(),\n",
        "                ])\n",
        "            self.head.append(nn.Linear(args.head_hidden[-1], args.classes))\n",
        "            self.head = nn.Sequential(*self.head)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
        "                m.momentum = args.bn_mom\n",
        "\n",
        "    #### GAP Bloc \n",
        "    #### input will be x (output of the densenet) and s (cell type onehot), \n",
        "    #### replaces x by its mean values and concatenates x to s, \n",
        "    #### gives (x+s) as input to neck,\n",
        "    #### returns the output of the neck ####\n",
        "    def embed(self, x, s):\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        #### replaces x by its mean values x.shape in output = [1,1] ####\n",
        "        #### flatten x : \n",
        "        x = x.view(x.size(0), -1)\n",
        "        #### if concat_cell_type = TRUE, concatenate x and s following (dim=1) (columns)\n",
        "        if self.concat_cell_type:\n",
        "            x = torch.cat([x, s], dim=1)\n",
        "\n",
        "        embedding = self.neck(x)\n",
        "        return embedding\n",
        "\n",
        "    def metric_classify(self, embedding):\n",
        "        return self.arc_margin_product(embedding)\n",
        "\n",
        "    def classify(self, embedding):\n",
        "        return self.head(embedding)\n",
        "\n",
        "\n",
        "class ModelAndLoss(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.model = Model(args)\n",
        "        self.metric_crit = ArcFaceLoss()\n",
        "        self.crit = DenseCrossEntropy()\n",
        "\n",
        "    def train_forward(self, x, s, y):\n",
        "        #### embedding = output of the neck \n",
        "        embedding = self.model.embed(x, s)\n",
        "\n",
        "        metric_output = self.model.metric_classify(embedding) #### returns arc_margin_product(embedding)\n",
        "        metric_loss = self.metric_crit(metric_output, y) #### ArcFaceLoss( arc_margin_product(embedding), y )\n",
        "\n",
        "        output = self.model.classify(embedding) #### output of the head \n",
        "        loss = self.crit(output, y)\n",
        "\n",
        "        #### accuracy calculation : calculates the mean value of good predictions of the model ####\n",
        "        #### output.max(1)[1] = index (of the column) of the maximum value of the row ####\n",
        "        acc = (output.max(1)[1] == y.max(1)[1]).float().mean().item()\n",
        "\n",
        "        #### returns the loss and the accuracy of the model #### \n",
        "        coeff = self.args.metric_loss_coeff\n",
        "        return loss * (1 - coeff) + metric_loss * coeff, acc\n",
        "\n",
        "    def eval_forward(self, x, s):\n",
        "        embedding = self.model.embed(x, s)\n",
        "        output = self.model.classify(embedding)\n",
        "        return output\n",
        "\n",
        "    def embed(self, x, s):\n",
        "        return self.model.embed(x, s)\n",
        "\n",
        "\n",
        "class DenseCrossEntropy(nn.Module):\n",
        "    def forward(self, x, target):\n",
        "        x = x.float()\n",
        "        target = target.float()\n",
        "        #### apply log_softmax to x following the dimension -1  (the last dimension, columns) <=> sum of the elements on each row = 1\n",
        "        logprobs = torch.nn.functional.log_softmax(x, dim=-1) \n",
        "        #### multiply (-logprobs) and target to get only the value of each correct class predicted \n",
        "        #### we add the values of the same row \n",
        "        #### we calculate the mean value of \"loss\"\n",
        "        loss = -logprobs * target\n",
        "        loss = loss.sum(-1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class ArcFaceLoss(nn.modules.Module):\n",
        "    def __init__(self, s=30.0, m=0.5):\n",
        "        super().__init__()\n",
        "        self.crit = DenseCrossEntropy()\n",
        "        self.s = s\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        logits = logits.float()\n",
        "        cosine = logits\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "\n",
        "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
        "        output *= self.s\n",
        "        loss = self.crit(output, labels)\n",
        "        return loss / 2\n",
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features): \n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    #### reset_parameters => normalise data ####\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        #### weight.shape = [out_features, in_features]\n",
        "        #### weight.size(1) => size of the dimension 1 => out_features\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        #### fill out weight with the values of its uniform distribution \n",
        "\n",
        "    def forward(self, features):\n",
        "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
        "        return cosine\n",
        "\n",
        "r = ArcMarginProduct(1024,1139)\n",
        "r.weight.dim()\n",
        "# Tensor creation\n",
        "t = torch.Tensor(3,2)\n",
        "#t = torch.Tensor([1,2]) * torch.tensor([0,1]) \n",
        "t = torch.Tensor( [[3,2],[1,0], [5,4]] )\n",
        "t\n",
        "t.max(1)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 1., 5.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20DT0jgyKjtQ",
        "outputId": "1b104b41-4fb2-4788-aac4-31bb3f0cca7d"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "test = torch.FloatTensor(3, 2)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.3658e-35, 0.0000e+00],\n",
              "        [1.5975e-43, 1.3873e-43],\n",
              "        [1.4574e-43, 1.6535e-43]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}