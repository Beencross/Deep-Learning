{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yvPlte6zbEo"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from apex import amp\n",
    "from torch import nn\n",
    "\n",
    "import dataset\n",
    "import precomputed as P\n",
    "from model import ModelAndLoss\n",
    "\n",
    "def parse_args():        ### function used for add_argument  ####\n",
    "    def lr_type(x):\n",
    "        x = x.split(',')\n",
    "        return x[0], list(map(float, x[1:]))\n",
    "\n",
    "    def bool_type(x):             #### function to determinate bool√©an type  ####\n",
    "        if x.lower() in ['1', 'true']:\n",
    "            return True\n",
    "        if x.lower() in ['0', 'false']:\n",
    "            return False\n",
    "        raise ValueError()   ### The raise statement is used to trigger a specific exception, \n",
    "                             ### A ValueError is a type of exception that is raised when an operation or function receives an argument with an inappropriate value assigned to it\n",
    "        \n",
    " #### The argparse module generates the help messages, the user manual, and removes errors when invalid arguments are given to the program ######\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "### The method add_argument() is used to specify which command line parameters the program can accept. ####    \n",
    "### It anticipates any potential problems that the program may encounter during the 3 phases of learning.   ###\n",
    "    parser.add_argument('-m', '--mode', default='train', choices=('train', 'val', 'predict'))\n",
    "    parser.add_argument('--backbone', default='mem-densenet161',\n",
    "            help='backbone for the architecture. '\n",
    "                 'Supported backbones: ResNets, ResNeXts, DenseNets (from torchvision), EfficientNets. '   ## help represents the error message in case of a problem\n",
    "                 'For DenseNets, add prefix \"mem-\" for memory efficient version')\n",
    "    parser.add_argument('--head-hidden', type=lambda x: None if not x else list(map(int, x.split(','))),   \n",
    "            help='hidden layers sizes in the head. Defaults to absence of hidden layers')\n",
    "    parser.add_argument('--concat-cell-type', type=bool_type, default=True)\n",
    "    parser.add_argument('--metric-loss-coeff', type=float, default=0.2)\n",
    "    \n",
    "    ### By default the command add_argument expects string arguments, so we have to specify the type when it is not the case. ######\n",
    "    cu\n",
    "    parser.add_argument('--embedding-size', type=int, default=1024)\n",
    "    parser.add_argument('--bn-mom', type=float, default=0.05)\n",
    "    parser.add_argument('--wd', '--weight-decay', type=float, default=1e-5)\n",
    "    parser.add_argument('--label-smoothing', '--ls', type=float, default=0)\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--mixup', type=float, default=0,\n",
    "            help='alpha parameter for mixup. 0 means no mixup')\n",
    "    parser.add_argument('--cutmix', type=float, default=1,\n",
    "            help='parameter for beta distribution. 0 means no cutmix')\n",
    "\n",
    "    parser.add_argument('--classes', type=int, default=1139,\n",
    "            help='number of classes predicting by the network')\n",
    "    parser.add_argument('--fp16', type=bool_type, default=True,\n",
    "            help='mixed precision training/inference')\n",
    "    parser.add_argument('--disp-batches', type=int, default=50,\n",
    "            help='frequency (in iterations) of printing statistics of training / inference '\n",
    "                 '(e.g. accuracy, loss, speed)')\n",
    "\n",
    "    parser.add_argument('--tta', type=int,\n",
    "            help='number of TTAs. Flips, 90 degrees rotations and resized crops (for --tta-size != 1) are applied')\n",
    "    parser.add_argument('--tta-size', type=float, default=1,\n",
    "            help='crop percentage for TTA')\n",
    "\n",
    "    parser.add_argument('--save',\n",
    "            help='path for the checkpoint with best accuracy. '\n",
    "                 'Checkpoint for each epoch will be saved with suffix .<number of epoch>')\n",
    "    parser.add_argument('--load',\n",
    "            help='path to the checkpoint which will be loaded for inference or fine-tuning')\n",
    "    parser.add_argument('--start-epoch', type=int, default=0)\n",
    "    parser.add_argument('--pred-suffix', default='',\n",
    "            help='suffix for prediction output. '\n",
    "                 'Predictions output will be stored in <loaded checkpoint path>.output<pred suffix>')\n",
    "\n",
    "    parser.add_argument('--pw-aug', type=lambda x: tuple(map(float, x.split(','))), default=(0.1, 0.1),\n",
    "            help='pixel-wise augmentation in format (scale std, bias std). scale will be sampled from N(1, scale_std) '\n",
    "                 'and bias from N(0, bias_std) for each channel independently')\n",
    "    parser.add_argument('--scale-aug', type=float, default=0.5,\n",
    "            help='zoom augmentation. Scale will be sampled from uniform(scale, 1). '\n",
    "                 'Scale is a scale for edge (preserving aspect)')\n",
    "    parser.add_argument('--all-controls-train', type=bool_type, default=True,\n",
    "            help='train using all control images (also these from the test set)')\n",
    "    parser.add_argument('--data-normalization', choices=('global', 'experiment', 'sample'), default='sample',\n",
    "            help='image normalization type: '\n",
    "                 'global -- use statistics from entire dataset, '\n",
    "                 'experiment -- use statistics from experiment, '\n",
    "                 'sample -- use mean and std calculated on given example (after normalization)')\n",
    "    parser.add_argument('--data', type=Path, default=Path('../data'),\n",
    "            help='path to the data root. It assumes format like in Kaggle with unpacked archives')\n",
    "    parser.add_argument('--cv-number', type=int, default=0, choices=(-1, 0, 1, 2, 3, 4, 5),\n",
    "            help='number of fold in 6-fold split. '\n",
    "                 'For number of given cell type experiment in certain fold see dataset.py file. '\n",
    "                 '-1 means not using validation set (training on all data)')\n",
    "    parser.add_argument('--data-split-seed', type=int, default=0,\n",
    "            help='seed for splitting experiments for folds')\n",
    "    parser.add_argument('--num-data-workers', type=int, default=10,\n",
    "            help='number of data loader workers')\n",
    "    parser.add_argument('--seed', type=int,\n",
    "            help='global seed (for weight initialization, data sampling, etc.). '\n",
    "                 'If not specified it will be randomized (and printed on the log)')\n",
    "\n",
    "    parser.add_argument('--pl-epoch', type=int, default=None,\n",
    "            help='first epoch where pseudo-labeling starts')\n",
    "    parser.add_argument('--pl-size-func', type=str, default='x',\n",
    "            help='function indicating percentage of the test set transferred to the training set. '\n",
    "                 'Function is called once an epoch and argument \"x\" is number from 0 to 1 indicating '\n",
    "                 'training progress (0 is first epoch of pseudo-labeling, and 1 is last epoch of traning). '\n",
    "                 'For example: \"x\" -- constant number of test examples is added each epoch; '\n",
    "                 '\"x*0.6+0.4\" -- 40% of test set added at the begining of pseudo-labesling and '\n",
    "                 'then constant number each epoch')\n",
    "\n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=24)\n",
    "    parser.add_argument('--gradient-accumulation', type=int, default=2,\n",
    "            help='number of iterations for gradient accumulation')\n",
    "    parser.add_argument('-e', '--epochs', type=int, default=90)\n",
    "    parser.add_argument('-l', '--lr', type=lr_type, default=('cosine', [1.5e-4]),\n",
    "            help='learning rate values and schedule given in format: schedule,value1,epoch1,value2,epoch2,...,value{n}. '\n",
    "                 'in epoch range [0, epoch1) initial_lr=value1, in [epoch1, epoch2) initial_lr=value2, ..., '\n",
    "                 'in [epoch{n-1}, total_epochs) initial_lr=value{n}, '\n",
    "                 'in every range the same learning schedule is used. Possible schedules: cosine, const')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == 'train':             ###The Assert instruction tests whether his condition is satisfied. If it is, it does nothing and if not, it immediately stops the execution of the program. ###\n",
    "        assert args.save is not None     ### Checks that the train mode is recorded ###\n",
    "    if args.mode == 'val':\n",
    "        assert args.save is None         ### Checks that the val mode is not recorded ###\n",
    "    if args.mode == 'predict':\n",
    "        assert args.load is not None\n",
    "        assert args.save is None         ### Checks thaht the predict mode is recorded #####\n",
    "\n",
    "    if args.seed is None:\n",
    "        args.seed = random.randint(0, 10 ** 9)    #### generates random numbers from 0 to 10 **9 ####\n",
    "\n",
    "    return args\n",
    "\n",
    "def setup_logging(args):       #### Args can take 2 value here : train or predict. This function permit to save result : it's like a file management ####\n",
    "    head = '{asctime}:{levelname}: {message}'\n",
    "    handlers = [logging.StreamHandler(sys.stderr)]\n",
    "    if args.mode == 'train':          ### If args concern train phase, add and create file with \"write mode\" and save it into the FileHandler ####\n",
    "        handlers.append(logging.FileHandler(args.save + '.log', mode='w'))\n",
    "    if args.mode == 'predict':\n",
    "        handlers.append(logging.FileHandler(args.load + '.output.log', mode='w'))\n",
    "    logging.basicConfig(level=logging.DEBUG, format=head, style='{', handlers=handlers)\n",
    "    logging.info('Start with arguments {}'.format(args))\n",
    "\n",
    "def setup_determinism(args):\n",
    "    torch.backends.cudnn.deterministic = True  ### cudnn only use deterministic convolution algorithms.\n",
    "    torch.backends.cudnn.benchmark = False    ####  When a cudnn convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, \n",
    "    ##benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. #####\n",
    "    \n",
    "    torch.manual_seed(args.seed)  ### Sets the seed for generating random numbers. Returns a torch.Generator object. ### \n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(args, model, loader): #### Infer and return prediction in dictionary formatted {sample_id: logits} ####\n",
    "    \n",
    "    if not len(loader):\n",
    "        return {}\n",
    "    res = {}               #### empty dictionary ####\n",
    "\n",
    "    model.eval()\n",
    "    tic = time.time()                #### The time() function returns the number of seconds passed since epoch. ####\n",
    "    for i, (X, S, I, *_) in enumerate(loader):\n",
    "        X = X.cuda()\n",
    "        S = S.cuda()\n",
    "\n",
    "        Xs = dataset.tta(args, X) if args.tta else [X]\n",
    "        ys = [model.eval_forward(X, S) for X in Xs]\n",
    "        y = torch.stack(ys).mean(0).cpu()\n",
    "\n",
    "        for j in range(len(I)):\n",
    "            assert I[j].item() not in res\n",
    "            res[I[j].item()] = y[j].numpy()\n",
    "\n",
    "        if (i + 1) % args.disp_batches == 0:\n",
    "            logging.info('Infer Iter: {:4d}  ->  speed: {:6.1f}'.format(\n",
    "                i + 1, args.disp_batches * args.batch_size / (time.time() - tic)))\n",
    "            tic = time.time()\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict(args, model):                 #### Entrypoint for predict mode ####\n",
    "\n",
    "    test_loader = dataset.get_test_loader(args)\n",
    "    train_loader, val_loader = dataset.get_train_val_loader(args, predict=True)\n",
    "\n",
    "    if args.fp16:\n",
    "        model = amp.initialize(model, opt_level='O1')\n",
    "\n",
    "    logging.info('Starting prediction')\n",
    "\n",
    "    output = {}\n",
    "    for k, loader in [('test', test_loader),\n",
    "                      ('val', val_loader)]:\n",
    "        output[k] = {}\n",
    "        res = infer(args, model, loader)\n",
    "\n",
    "        for i, v in res.items():\n",
    "            d = loader.dataset.data[i]\n",
    "            name = '{}_{}_{}'.format(d[0], d[1], d[2])\n",
    "            if name not in output[k]:\n",
    "                output[k][name] = []\n",
    "            output[k][name].append(v)\n",
    "\n",
    "    logging.info('Saving predictions to {}'.format(args.load + '.output' + args.pred_suffix))\n",
    "    with open(args.load + '.output' + args.pred_suffix, 'wb') as file:\n",
    "        pickle.dump(output, file)\n",
    "\n",
    "\n",
    "def score(args, model, loader):        #### Return accuracy of the model on validation set #####\n",
    "    \n",
    "    logging.info('Starting validation')\n",
    "\n",
    "    res = infer(args, model, loader)\n",
    "\n",
    "    cell_type_c = np.array([0, 0, 0, 0])  ##### number of examples for given cell type ###\n",
    "    cell_type_s = np.array([0, 0, 0, 0])  #### number of correctly classified examples for given cell type ####\n",
    "    for i, v in res.items():\n",
    "        d = loader.dataset.data[i]\n",
    "        r = v[:loader.dataset.treatment_classes].argmax() == d[-1]\n",
    "\n",
    "        ser = loader.dataset.cell_types.index(d[4])\n",
    "        cell_type_c[ser] += 1\n",
    "        cell_type_s[ser] += r\n",
    "\n",
    "    acc = (cell_type_s.sum() / cell_type_c.sum()).item() if cell_type_c.sum() != 0 else 0\n",
    "    logging.info('Eval: acc: {} ({})'.format(cell_type_s / cell_type_c, acc))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_learning_rate(args, epoch):           ##### function who define Learning Rate\n",
    "    assert len(args.lr[1][1::2]) + 1 == len(args.lr[1][::2])\n",
    "    for start, end, lr, next_lr in zip([0] + args.lr[1][1::2],\n",
    "                                       args.lr[1][1::2] + [args.epochs],\n",
    "                                       args.lr[1][::2],\n",
    "                                       args.lr[1][2::2] + [0]):\n",
    "        if start <= epoch < end:\n",
    "            if args.lr[0] == 'cosine':\n",
    "                return lr * (math.cos((epoch - start) / (end - start) * math.pi) + 1) / 2\n",
    "            elif args.lr[0] == 'const':\n",
    "                return lr\n",
    "            else:\n",
    "                assert 0\n",
    "    assert 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_label(args, Y):\n",
    "    nY = nn.functional.one_hot(Y, args.classes).float()\n",
    "    nY += args.label_smoothing / (args.classes - 1)\n",
    "    nY[range(Y.size(0)), Y] -= args.label_smoothing / (args.classes - 1) + args.label_smoothing\n",
    "    return nY\n",
    "\n",
    "@torch.no_grad()\n",
    "def transform_input(args, X, S, Y):    #### Apply mixup, cutmix, and label-smoothing ####\n",
    "\n",
    "    Y = smooth_label(args, Y)\n",
    "\n",
    "    if args.mixup != 0 or args.cutmix != 0:\n",
    "        perm = torch.randperm(args.batch_size).cuda()\n",
    "        \n",
    "        ### CutMix is an augmentation technique witch cut and paste random patches between the training images. The right labels are mixed in proportion to the area of patches in the images. CutMix increases localization ability by making the model to focus on less discriminative parts of the image being classified\n",
    "        ### The right labels are mixed in proportion to the area of patches in the images. ###\n",
    "        ### CutMix increases localization ability by making the model to focus on less discriminative parts of the image being classified  ###\n",
    "        \n",
    "    if args.mixup != 0:\n",
    "        coeffs = torch.tensor(np.random.beta(args.mixup, args.mixup, args.batch_size), dtype=torch.float32).cuda()\n",
    "        X = coeffs.view(-1, 1, 1, 1) * X + (1 - coeffs.view(-1, 1, 1, 1)) * X[perm,]\n",
    "        S = coeffs.view(-1, 1) * S + (1 - coeffs.view(-1, 1)) * S[perm,]\n",
    "        Y = coeffs.view(-1, 1) * Y + (1 - coeffs.view(-1, 1)) * Y[perm,]\n",
    "\n",
    "    if args.cutmix != 0:\n",
    "        img_height, img_width = X.size()[2:]\n",
    "        lambd = np.random.beta(args.cutmix, args.cutmix)\n",
    "        column = np.random.uniform(0, img_width)\n",
    "        row = np.random.uniform(0, img_height)\n",
    "        height = (1 - lambd) ** 0.5 * img_height\n",
    "        width = (1 - lambd) ** 0.5 * img_width\n",
    "        r1 = round(max(0, row - height / 2))\n",
    "        r2 = round(min(img_height, row + height / 2))\n",
    "        c1 = round(max(0, column - width / 2))\n",
    "        c2 = round(min(img_width, column + width / 2))\n",
    "        if r1 < r2 and c1 < c2:\n",
    "            X[:, :, r1:r2, c1:c2] = X[perm, :, r1:r2, c1:c2]\n",
    "\n",
    "            lambd = 1 - (r2 - r1) * (c2 - c1) / (img_height * img_width)\n",
    "            S = S * lambd + S[perm] * (1 - lambd)\n",
    "            Y = Y * lambd + Y[perm] * (1 - lambd)\n",
    "\n",
    "    return X, S, Y\n",
    "\n",
    "def pseudo_label(args, epoch, pl_data, model, val_loader, test_loader, train_loader): ### Pseudo-label some test and validation examples and move them to the training set ###\n",
    "\n",
    "    if args.pl_epoch is None or epoch < args.pl_epoch:\n",
    "        return\n",
    "\n",
    "    logging.info('Starting pseudo-labeling')\n",
    "\n",
    "    test_loader.dataset.filter(lambda i, d: ('test', i) not in pl_data)\n",
    "    test_res = infer(args, model, test_loader)\n",
    "    test_loader.dataset.filter()\n",
    "\n",
    "    val_loader.dataset.filter(lambda i, d: ('val', i) not in pl_data)\n",
    "    val_res = infer(args, model, val_loader)\n",
    "    val_loader.dataset.filter()\n",
    "\n",
    "    test_res = sorted(test_res.items())\n",
    "    val_res = sorted(val_res.items())\n",
    "\n",
    "\n",
    "    set_classes = defaultdict(lambda: [])  # classes that are already in the training set for the plate\n",
    "    for j in range(len(train_loader.dataset.data)):\n",
    "        experiment_plate = train_loader.dataset.data[j][:2]\n",
    "        sirna = train_loader.dataset.data[j][-1]\n",
    "        set_classes[experiment_plate].append(sirna)\n",
    "\n",
    "    confs = []\n",
    "    last = None\n",
    "    for k, (i, v) in itertools.chain(\n",
    "            zip(itertools.repeat('val'), val_res),\n",
    "            zip(itertools.repeat('test'), test_res)):\n",
    "        loader = val_loader if k == 'val' else test_loader\n",
    "\n",
    "        # assumes that both sides of an example will be next to each other\n",
    "        if i % 2 == 0:\n",
    "            assert last is None\n",
    "            last = i, v\n",
    "            continue\n",
    "        else:\n",
    "            last_i, last_v = last\n",
    "            assert last_i == i - 1\n",
    "            last = None\n",
    "\n",
    "            logits = v + last_v  # ensemble two sites\n",
    "            plate = loader.dataset.data[i][1] - 1\n",
    "            experiment = loader.dataset.data[i][0]\n",
    "            class_group_id = P.group_assignment[experiment][plate]\n",
    "            possible_classes = P.groups[class_group_id]\n",
    "            remaining_classes = list(set(range(loader.dataset.treatment_classes)) - possible_classes)\n",
    "            logits[remaining_classes] = -10e6\n",
    "\n",
    "            experiment_plate = loader.dataset.data[i][:2]\n",
    "            if set_classes[experiment_plate]:\n",
    "                logits[set_classes[experiment_plate]] = -10e6\n",
    "            logits = logits[:loader.dataset.treatment_classes]\n",
    "            r = logits.argmax().item()\n",
    "\n",
    "            logits.sort()\n",
    "            c = logits[-1] - logits[-2]\n",
    "            confs.append(((k, i - 1), c, r))\n",
    "\n",
    "\n",
    "    x = (epoch - args.pl_epoch + 1) / (args.epochs - args.pl_epoch + 1)\n",
    "    val_test_examples = len(val_loader.dataset.data) // 2 + len(test_loader.dataset.data) // 2\n",
    "    added_examples = len(pl_data) // 2\n",
    "    n = round(eval('lambda x: ' + args.pl_size_func)(x) * val_test_examples) - added_examples\n",
    "    n = max(n, 0)\n",
    "\n",
    "    confs = list(filter(lambda x: x[0] not in pl_data, confs))\n",
    "    confs.sort(key=lambda x: -x[1])\n",
    "    confs = confs[:n]\n",
    "\n",
    "    val_misclass = 0\n",
    "    val_count = 0\n",
    "    test_count = 0\n",
    "    not_added_count = 0\n",
    "    added_sirnas = defaultdict(set)\n",
    "    for (k, i), c, r in confs:\n",
    "        if k == 'val':\n",
    "            d1 = val_loader.dataset.data[i]\n",
    "            d2 = val_loader.dataset.data[i + 1]\n",
    "        elif k == 'test':\n",
    "            d1 = test_loader.dataset.data[i]\n",
    "            d2 = test_loader.dataset.data[i + 1]\n",
    "        else:\n",
    "            assert 0\n",
    "        assert d1[:3] == d2[:3] and d1[-2:] == d2[-2:]\n",
    "\n",
    "        if r in added_sirnas[d1[:2]]:\n",
    "            not_added_count += 1\n",
    "            continue\n",
    "\n",
    "        if k == 'val':\n",
    "            val_count += 1\n",
    "            if d1[-1] != r:\n",
    "                val_misclass += 1\n",
    "        elif k == 'test':\n",
    "            test_count += 1\n",
    "        else:\n",
    "            assert 0\n",
    "\n",
    "        added_sirnas[d1[:2]].add(r)\n",
    "        pl_data.add((k, i))\n",
    "        pl_data.add((k, i + 1))\n",
    "        train_loader.dataset.data.append((*d1[:-1], r))\n",
    "        train_loader.dataset.data.append((*d2[:-1], r))\n",
    "\n",
    "    logging.info('Pseudo-labeling: Added {} ({} val, {} test), {} ({:.3f}%) val misclassified, '\n",
    "                 '{} ({:.3f}%) not added, pl_data size {}, train size {}, threshold {}'.format(\n",
    "                     n, val_count, test_count, val_misclass, val_misclass / val_count * 100 if val_count != 0 else 0,\n",
    "                     not_added_count, not_added_count / (not_added_count + n) * 100 if not_added_count + n != 0 else 0,\n",
    "                     len(pl_data), len(train_loader.dataset.data), confs[-1][1] if len(confs) != 0 else 'None'))\n",
    "\n",
    "\n",
    "def train(args, model):\n",
    "    train_loader, val_loader = dataset.get_train_val_loader(args)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0, weight_decay=args.wd)    #### weight decay is a regularization technique used to limit overlearning. ####\n",
    "                                                                                    #### It consists in adding a penalty to the error function that depends on the weights that link the neurons together. ###\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')         #### Allow Amp to perform casts as required by the opt_level ####\n",
    "\n",
    "    if args.load is not None:\n",
    "        best_acc = score(args, model, val_loader)\n",
    "    else:\n",
    "        best_acc = float('-inf')\n",
    "\n",
    "    if args.mode == 'val':\n",
    "        return\n",
    "\n",
    "    if args.pl_epoch is not None:\n",
    "        test_loader = dataset.get_test_loader(args, exclude_leak=True)\n",
    "        pl_data = set()\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.pl_epoch is not None:\n",
    "            pseudo_label(args, epoch, pl_data, model, val_loader, test_loader, train_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_norm = np.mean([v.norm().item() for v in model.parameters()])\n",
    "\n",
    "        logging.info('Train: epoch {}   avg_norm: {}'.format(epoch, avg_norm))\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cum_loss = 0\n",
    "        cum_acc = 0\n",
    "        cum_count = 0\n",
    "        tic = time.time()\n",
    "        for i, (X, S, _, Y) in enumerate(train_loader):\n",
    "            lr = get_learning_rate(args, epoch + i / len(train_loader))\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "\n",
    "            X = X.cuda()\n",
    "            S = S.cuda()\n",
    "            Y = Y.cuda()\n",
    "            X, S, Y = transform_input(args, X, S, Y)\n",
    "\n",
    "            loss, acc = model.train_forward(X, S, Y)\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            if (i + 1) % args.gradient_accumulation == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            cum_count += 1\n",
    "            cum_loss += loss.item()\n",
    "            cum_acc += acc\n",
    "            if (i + 1) % args.disp_batches == 0:\n",
    "                logging.info('Epoch: {:3d} Iter: {:4d}  ->  speed: {:6.1f}   lr: {:.9f}   loss: {:.6f}   acc: {:.6f}'.format(\n",
    "                    epoch, i + 1, cum_count * args.batch_size / (time.time() - tic), optimizer.param_groups[0]['lr'],\n",
    "                    cum_loss / cum_count, cum_acc / cum_count))\n",
    "                cum_loss = 0\n",
    "                cum_acc = 0\n",
    "                cum_count = 0\n",
    "                tic = time.time()\n",
    "\n",
    "        acc = score(args, model, val_loader)\n",
    "        torch.save(model.state_dict(), str(args.save + '.{}'.format(epoch)))\n",
    "        if acc >= best_acc:\n",
    "            best_acc = acc\n",
    "            logging.info('Saving best to {} with score {}'.format(args.save, best_acc))\n",
    "            torch.save(model.state_dict(), str(args.save))\n",
    "\n",
    "def main(args):\n",
    "    model = ModelAndLoss(args).cuda()\n",
    "    logging.info('Model:\\n{}'.format(str(model)))\n",
    "\n",
    "    if args.load is not None:\n",
    "        logging.info('Loading model from {}'.format(args.load))\n",
    "        model.load_state_dict(torch.load(str(args.load)))\n",
    "\n",
    "    if args.mode in ['train', 'val']:\n",
    "        train(args, model)\n",
    "    elif args.mode == 'predict':\n",
    "        predict(args, model)\n",
    "    else:\n",
    "        assert 0\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    setup_logging(args)\n",
    "    setup_determinism(args)\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
